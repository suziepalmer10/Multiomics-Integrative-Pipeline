---
title: "Feature Comparison Analysis in R"
output: html_notebook
date: 03-13-2025 (last modified)
author: Suzette Palmer
---

Code to run before: 

This code is used for comparison analyses between the features (metabolites and taxa) selected by Elastic Net, XGBoost and Random Forest Base Models. This code is used for Figures 4-6 and Supplemental Figures __. 

First, install the following packages. To do this, you can Google "install" and name of the package. 
For example, for the "fill_pattern" package, you can type in: "#install.packages("fillpattern")" to your RStudio Console. 
Note: Package versions can be found below. 
```{r}
#Load Libraries
library(ggplot2) #ggplot2_3.5.1
library(reshape2) #reshape2_1.4.4
library(fillpattern) #fillpattern_1.0.2
library(scales) #scales_1.3.0
library(writexl) #writexl_1.4.2
```
This next code block will allow the user to decide how many features should be compared between the Elastic Net, Random Forest and XGBoost Models. For this analysis, I explored comparisons between the top feature, top 3, top 5, top 10, top 20 and top 50 features. When I say "top feature", I mean the features that have the greatest importance for each of the models. 
Note: If you would like to use the code below for your own analysis, you will need to put in an output path for your files to be stored and change the number of comparisons (non_zero_count_value) to whatever number you desire. 
If you want to compare the top 20 features selected for each model, you would use "20" as your number (example below). 
You will also need to change the working directory to whereever you store your data. 
```{r}
#setwd ("/Users/suzettepalmer/Desktop/IntegratedLearner_March2025/Code_and_Files/Feature_Comparisons/")

#Top 20
#non_zero_count_value = 20
#output_path = 'Top20/'

# #Top 1
 # non_zero_count_value = 1
 # output_path = 'Top1/'
# #Top 5
# non_zero_count_value = 5
# output_path = 'Top5/'
# #Top 10
# non_zero_count_value = 10
# output_path = 'Top10/'
# #Top 50
# non_zero_count_value = 50
# output_path = 'Top50/'
# #Top 3
non_zero_count_value = 3
output_path = 'Top3/'
```
This next block contains multiple functions. 

The first function, "process_files_by_pattern" extracts files based on a pattern found at the end of each filename.

As an example: "era_metab_results <- process_files_by_pattern("metab_importance\\.csv$", '/Users/suzettepalmer/Desktop/IntegratedLearner_Dec2024/Coefficients/Era_feature_extraction')" will extract all files that end with "metab_importance.csv" that are contained in the directory '/Users/suzettepalmer/Desktop/IntegratedLearner_Dec2024/Coefficients/Era_feature_extraction'. Each file contains feature importance that is read in as a dataframe and placed into a list called "data_list". Each list is named after the file_name. 

Next, each dataframe contained in the list, "data_list" is fed through the "processed_data_list" function. The feature name column and feature importance column are extracted and sorted in descending order based on the importance value. NA values are next checked and removed. Next, if the number of features is for the dataset is greater than 0 and to make sure the number of selected features is greater or equal to the comparison threshold ("non_zero_count") provided by the user. All dataframes in the list are processed and returned to the user in a list. For the example above, "era_metab_results" contains these processed files.
```{r}
# Store datasets with fewer than n non-zero values dynamically (n is defined by non_zero_count_value defined by you above)
remove_names <- c()


process_files_by_pattern <- function(pattern, directory = ".", feature_column = "Feature", importance_column = "Mean_Importance") {
  # Get a list of all files matching the pattern
  files <- list.files(path = directory, pattern = pattern, recursive = TRUE, full.names = TRUE)
  
  # Load all files into a list
  data_list <- lapply(files, read.csv)
  
  # Name the list elements with the corresponding filenames
  file_names <- basename(files)
  names(data_list) <- file_names
  
  # Process each dataframe by filtering the dataframe by importance values, removing NAs and counting the number of non-zero values. 
  processed_data_list <- lapply(names(data_list), function(file) {
    df <- data_list[[file]]
    if (all(c(feature_column, importance_column) %in% colnames(df))) {
      # Filter and sort the dataframe
      df <- df[order(-df[[importance_column]]), c(feature_column, importance_column), drop = FALSE]
      
      # Remove NA values from the importance column
      df <- df[!is.na(df[[importance_column]]), ]
      
      # Count non-zero values
      non_zero_count <- sum(df[[importance_column]] > 0, na.rm = TRUE)
      if (non_zero_count < non_zero_count_value) {
        remove_names <<- c(remove_names, basename(file))
      }
      
      return(df)
    } else {
      warning(paste("Required columns not found in:", file))
      return(NULL)
    }
  })
  
  # Assign file names to the list names
  processed_data_list <- setNames(processed_data_list, names(data_list))
  
  return(processed_data_list)
}

```
These commands go with the code chunk above to allow us to extract all of the dataframe files from each of the analyses run and that we want to graph in future R markdown chunks. 
```{r}
setwd ("/Users/suzettepalmer/Desktop/Integrated_Pipeline_March2025/Code_and_Analyses/Manuscript_Code/C_Feature_Importance_Values/Output_FeatureExtraction")
# Erawijantari
era_metab_results <- process_files_by_pattern("metab_importance\\.csv$", 'Era_feature_extraction')
era_taxa_results <- process_files_by_pattern("taxa_importance\\.csv$", 'Era_feature_extraction')
era_concat_results <- process_files_by_pattern("concat_importance\\.csv$", 'Era_feature_extraction')
# Franzosa
franzosa_metab_results <- process_files_by_pattern("metab_importance\\.csv$", 'Franzosa_feature_extraction')
franzosa_taxa_results <- process_files_by_pattern("taxa_importance.csv", 'Franzosa_feature_extraction')
franzosa_concat_results <- process_files_by_pattern("concat_importance.csv", 'Franzosa_feature_extraction')
#Yachida
yachida_metab_results <- process_files_by_pattern("metab_importance\\.csv$", 'Yachida_feature_extraction')
yachida_taxa_results <- process_files_by_pattern("taxa_importance.csv", 'Yachida_feature_extraction')
yachida_concat_results <- process_files_by_pattern("concat_importance.csv", 'Yachida_feature_extraction')
#Wang
wang_metab_results <- process_files_by_pattern("metab_importance\\.csv$", 'Wang_feature_extraction')
wang_taxa_results <- process_files_by_pattern("taxa_importance.csv", 'Wang_feature_extraction')
wang_concat_results <- process_files_by_pattern("concat_importance.csv", 'Wang_feature_extraction')
```
Since my feature extraction results are stored based on Dataset name in separate directories, I need to separate my feature reduced results from my full dimensional results (represented as 'nf_' or 'un'). As an example, we have this line: "era_metab_split <- split_dataframes(era_metab_results, '^era', '^nf')", which uses the "split_dataframes" function. The feature reduced files start with "era" and the full dimensional files start with "nf". This will return a list of two lists to "era_metab_split", with the first being the feature reduced ("filtered") and the second being the full dimensional ("unfiltered) list. 
```{r}
#split filtered and unfiltered dataframes
split_dataframes <- function(data_list, prefix_era, prefix_nf = "^nf") {
  # Ensure the list has names
  names_list <- names(data_list)
  # Split the data based on the prefixes
  filtered_df <- data_list[grepl(prefix_era, names_list)]
  unfiltered_df <- data_list[grepl(prefix_nf, names_list)]
  # Return a list with the two split datasets
  return(list(filtered = filtered_df, unfiltered = unfiltered_df))
}
```
The chunk below goes with the above function and separates filtered and unfiltered data for downstream analyses. 
```{r}
#era
era_metab_split <- split_dataframes(era_metab_results, '^era', '^nf')
era_taxa_split <- split_dataframes(era_taxa_results, '^era', '^nf')
era_concat_split <- split_dataframes(era_concat_results, '^era', '^nf')
#franzosa
franz_metab_split <- split_dataframes(franzosa_metab_results, '^franzosa', '^un')
franz_taxa_split <- split_dataframes(franzosa_taxa_results, '^franzosa', '^un')
franz_concat_split <- split_dataframes(franzosa_concat_results, '^franzosa', '^un')
#yachida
yachida_metab_split <- split_dataframes(yachida_metab_results, '^yachida', '^nf')
yachida_taxa_split <- split_dataframes(yachida_taxa_results, '^yachida', '^nf')
yachida_concat_split <- split_dataframes(yachida_concat_results, '^yachida', '^nf')
#wang
wang_metab_split <- split_dataframes(wang_metab_results, '^wang', '^nf')
wang_taxa_split <- split_dataframes(wang_taxa_results, '^wang', '^nf')
wang_concat_split <- split_dataframes(wang_concat_results, '^wang', '^nf')

```
These functions combine the feature extraction outputs for the four studies. Next the feature reduced ('filtered') and full dimensional ('unfiltered') lists are extracted and placed in new lists that contain only files of their same type (i.e., filtered or unfiltered).  
```{r}
#metabolomics filtered and unfiltered
metab_df <- list(era_metab_split, franz_metab_split, yachida_metab_split, wang_metab_split)
metab_filtered_list <- do.call(c, lapply(metab_df, function(x) x$filtered))
metab_unfiltered_list <- do.call(c, lapply(metab_df, function(x) x$unfiltered))
#taxa unfiltered and filtered list
taxa_df <- list(era_taxa_split, franz_taxa_split, yachida_taxa_split, wang_taxa_split)
taxa_filtered_list <- do.call(c, lapply(taxa_df, function(x) x$filtered))
taxa_unfiltered_list <- do.call(c, lapply(taxa_df, function(x) x$unfiltered))
#concat unfiltered and filtered list
concat_df <- list(era_concat_split, franz_concat_split, yachida_concat_split, wang_concat_split)
concat_filtered_list <- do.call(c, lapply(concat_df, function(x) x$filtered))
concat_unfiltered_list <- do.call(c, lapply(concat_df, function(x) x$unfiltered))
```
This provides the name of the files that did survive the "process_files_by_pattern" function due to NA values and/or not enough features (such as 20) with feature importance values greater than 0.  
```{r}
# Print the names of datasets that will be removed
if (length(remove_names) > 0) {
  message("Datasets with fewer than Desired non-zero values in 'Mean_Importance' and will be removed:")
  print(remove_names)
}
```
Dataframes that had been stored in remove names are now removed. 
```{r}
# Filter the list to exclude the specified dataframes
metab_filtered_list1 <- metab_filtered_list[!(names(metab_filtered_list) %in% remove_names)]
metab_unfiltered_list1 <- metab_unfiltered_list[!(names(metab_unfiltered_list) %in% remove_names)]
taxa_filtered_list1 <- taxa_filtered_list[!(names(taxa_filtered_list) %in% remove_names)]
taxa_unfiltered_list1 <- taxa_unfiltered_list[!(names(taxa_unfiltered_list) %in% remove_names)]
concat_filtered_list1 <- concat_filtered_list[!(names(concat_filtered_list) %in% remove_names)]
concat_unfiltered_list1 <- concat_unfiltered_list[!(names(concat_unfiltered_list) %in% remove_names)]
```
The "extract_top_features" function selects only the highest ranked features based on the users "non_zero_count_value", which is the number of desired features that the user would like to compare between models. 
```{r}
# Function to extract top desired features from a list of dataframes for comparison
extract_top_features <- function(dataframes_list, feature_num) {
  top_features_list <- lapply(names(dataframes_list), function(name) {
    top_20 <- dataframes_list[[name]]$Feature[1:feature_num] # Extract top 20 features
  })
  # Name each list element with the original dataframe name
  names(top_features_list) <- names(dataframes_list)
  return(top_features_list)
}

```
This code chunk uses the function defined above to extract the top features for the datasets we are using below. 
```{r}
metab_filtered_list2 <- extract_top_features(metab_filtered_list1, non_zero_count_value)
metab_unfiltered_list2 <- extract_top_features(metab_unfiltered_list1, non_zero_count_value)
taxa_filtered_list2 <- extract_top_features(taxa_filtered_list1, non_zero_count_value)
taxa_unfiltered_list2 <- extract_top_features(taxa_unfiltered_list1, non_zero_count_value)
concat_filtered_list2 <- extract_top_features(concat_filtered_list1, non_zero_count_value)
concat_unfiltered_list2 <- extract_top_features(concat_unfiltered_list1, non_zero_count_value)
```

```{r}
# Convert list of lists to a dataframe with names as identifiers
list_to_dataframe <- function(results_list) {
  # Remove lists containing NA values
  valid_results <- results_list[!sapply(results_list, function(x) any(is.na(unlist(x))))]
  
  # Get names of valid lists
  valid_names <- names(valid_results)
  
  # Convert to dataframe
  df <- do.call(rbind, lapply(seq_along(valid_results), function(i) {
    cbind(Dataframe_Name = valid_names[i], as.data.frame(valid_results[[i]]))
  }))
  
  return(df)
}

separate_lists_by_pattern <- function(data_list, pattern) {
  # Filter the list based on the given pattern in the names
  matched_list <- data_list[grepl(pattern, names(data_list))]
  return(matched_list)
}
```
Function to perform model comparisons based on the user's desired number of features. This will calculate proportions of overlap and uniquely detected features for Elastic Net, Random Forest and XGBoost. The output will be an excel files with these proportions. 
```{r}
# Function to perform model comparisons based on the user's desired number of features. 
automate_comparisons <- function(output_path, patterns, filtered_list, file_name) {
  # Separate lists based on patterns
  separated_lists <- lapply(patterns, function(p) separate_lists_by_pattern(filtered_list, paste0("^", p)))
  # Function to compute detailed comparisons
  compute_detailed_comparisons <- function(lists) {
    if (length(lists) != 3) return(NA)  # Skip if lists do not have exactly 3 elements
    
    enet <- lists[[1]]
    rf <- lists[[2]]
    xgb <- lists[[3]]
    
    format_count <- function(count) round(count / non_zero_count_value, 2)
    
    common_all <- intersect(intersect(enet, rf), xgb)
    enet_rf_overlap <- setdiff(intersect(enet, rf), xgb)
    enet_xgb_overlap <- setdiff(intersect(enet, xgb), rf)
    rf_xgb_overlap <- setdiff(intersect(rf, xgb), enet)
    
    enet_only <- setdiff(enet, union(rf, xgb))
    rf_only <- setdiff(rf, union(enet, xgb))
    xgb_only <- setdiff(xgb, union(enet, rf))
    
    list(
      all_overlap = format_count(length(common_all)),
      enet_rf_overlap = format_count(length(enet_rf_overlap)),
      enet_xgb_overlap = format_count(length(enet_xgb_overlap)),
      rf_xgb_overlap = format_count(length(rf_xgb_overlap)),
      enet_only = format_count(length(enet_only)),
      rf_only = format_count(length(rf_only)),
      xgb_only = format_count(length(xgb_only))
    )
  }
  # Apply the function to each separated list
  comparisons_results <- lapply(separated_lists, compute_detailed_comparisons)
  names(comparisons_results) <- patterns
  
  # Function to convert the result list to a dataframe
  convert_list_to_dataframe <- function(overlap_list) {
    data.frame(
      Method = c("all_overlap", "enet_rf", "enet_xgb", "rf_xgb", "enet", "rf", "xgb"),
      enet = c(overlap_list$all_overlap, overlap_list$enet_rf_overlap, overlap_list$enet_xgb_overlap, 0, overlap_list$enet_only, 0, 0),
      rf   = c(overlap_list$all_overlap, overlap_list$enet_rf_overlap, 0, overlap_list$rf_xgb_overlap, 0, overlap_list$rf_only, 0),
      xgb  = c(overlap_list$all_overlap, 0, overlap_list$enet_xgb_overlap, overlap_list$rf_xgb_overlap, 0, 0, overlap_list$xgb_only)
    )
  }
  for (name in names(comparisons_results)) {
    overlap_list <- comparisons_results[[name]]
    if (is.null(overlap_list) || all(is.na(overlap_list))) next  # Skip invalid results
    df <- convert_list_to_dataframe(overlap_list)
    #generate_and_save_plot(df, name)
  }
  comparisons_results <- list_to_dataframe(comparisons_results)
  writexl::write_xlsx(comparisons_results, path = paste0(output_path, non_zero_count_value, file_name, sep=''))
  return(comparisons_results)
}
```

```{r}
extract_unique_patterns <- function(name_list) {
  # Define regex pattern to extract portion before numbers or model names
  pattern <- "^(.*?)(?:_\\d+|_enet|_xgb|_xgboost|_rf).*"
  # Extract the relevant part of the name
  extracted <- sub(pattern, "\\1", name_list)
  # Keep unique values
  unique_patterns <- unique(extracted)
  return(unique_patterns)
}
```

```{r}
setwd("/Users/suzettepalmer/Desktop/Integrated_Pipeline_March2025/Code_and_Analyses/Manuscript_Code/D_Feature_Comparisons/Output_FeatureComparison")
dir.create(output_path, showWarnings = FALSE, recursive = TRUE)
#pattern extraction for filtered metabolomics
fm_patterns <- extract_unique_patterns(names(metab_filtered_list2))
metab_filtered_results_list <- automate_comparisons(output_path,fm_patterns, metab_filtered_list2, "metab_filtered_03112025.xlsx")
#pattern extraction for unfiltered metabolomics
um_patterns <- extract_unique_patterns(names(metab_unfiltered_list2))
metab_unfiltered_results_list <- automate_comparisons(output_path, um_patterns, metab_unfiltered_list2, "metab_unfiltered_03112025.xlsx")
#pattern for filtered taxa
ft_patterns <- extract_unique_patterns(names(taxa_filtered_list2))
taxa_filtered_results_list <- automate_comparisons(output_path,ft_patterns, taxa_filtered_list2, "taxa_filtered_03112025.xlsx")
#pattern for unfiltered taxa
ut_patterns <- extract_unique_patterns(names(taxa_unfiltered_list2))
taxa_filtered_results_list <- automate_comparisons(output_path, ut_patterns, taxa_unfiltered_list2, "taxa_unfiltered_03112025.xlsx")
#pattern extraction for filtered concatenation
cf_patterns <- extract_unique_patterns(names(concat_filtered_list2))
concat_filtered_results_list <- automate_comparisons(output_path, cf_patterns, concat_filtered_list2, "concat_filtered_03112025.xlsx")
#pattern extraction for unfiltered concatenation
uc_patterns <- extract_unique_patterns(names(concat_unfiltered_list2))
concat_unfiltered_results_list <- automate_comparisons(output_path, uc_patterns, concat_unfiltered_list2, "concat_unfiltered_03112025.xlsx")

```

Code to run after: 

